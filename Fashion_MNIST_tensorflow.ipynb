{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYjGX62I9tpT1njefkKqsj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiyichanmyae/keras/blob/master/Fashion_MNIST_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "id": "aOn2yE66r3fI",
        "outputId": "dd44614f-444b-4185-d4c8-4e5a9be7db86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " X train's shape : (60000, 28, 28)\n",
            " Y tain's shape  : (60000,)\n",
            "LABEL: 9\n",
            "\n",
            "IMAGE PIXEL ARRAY:\n",
            " [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f76ccc60b20>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "fmnist = keras.datasets.fashion_mnist\n",
        "\n",
        "# Load the training and test split of the Fashion MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = fmnist.load_data()\n",
        "print(' X train\\'s shape :' , X_train.shape)\n",
        "print(' Y tain\\'s shape  :' , y_train.shape)\n",
        "\n",
        "# You can put between 0 and 59999\n",
        "index=0\n",
        "# Set number of characters per row when printing\n",
        "np.set_printoptions(linewidth=320)\n",
        "# Print the label and image\n",
        "print(f'LABEL: {y_train[index]}')\n",
        "print(f'\\nIMAGE PIXEL ARRAY:\\n {X_train[index]}')\n",
        "# Visualize the image\n",
        "plt.imshow(X_train[index])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lUcWaiX7MFj"
      },
      "source": [
        "[Sequential](https://keras.io/api/models/sequential/): That defines a sequence of layers in the neural network.\n",
        "\n",
        "[Flatten](https://keras.io/api/layers/reshaping_layers/flatten/): Remember earlier where our images were a 28x28 pixel matrix when you printed them out? Flatten just takes that square and turns it into a 1-dimensional array.\n",
        "\n",
        "[Dense](https://keras.io/api/layers/core_layers/dense/): Adds a layer of neurons\n",
        "\n",
        "Each layer of neurons need an [activation function](https://keras.io/api/layers/activations/) to tell them what to do. There are a lot of options, but just use these for now: \n",
        "\n",
        "[ReLU](https://keras.io/api/layers/activations/#relu-function) effectively means:\n",
        "\n",
        "```\n",
        "if x > 0: \n",
        "  return x\n",
        "\n",
        "else: \n",
        "  return 0\n",
        "```\n",
        "\n",
        "In other words, it only passes values 0 or greater to the next layer in the network.\n",
        "\n",
        "[Softmax](https://keras.io/api/layers/activations/#softmax-function) takes a list of values and scales these so the sum of all elements will be equal to 1. When applied to model outputs, you can think of the scaled values as the probability for that class. For example, in your classification model which has 10 units in the output dense layer, having the highest value at `index = 4` means that the model is most confident that the input clothing image is a coat. If it is at index = 5, then it is a sandal, and so forth. See the short code block below which demonstrates these concepts. You can also watch this [lecture](https://www.youtube.com/watch?v=LLux1SW--oM&ab_channel=DeepLearningAI) if you want to know more about the Softmax function and how the values are computed.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the pixel values of the train and test images\n",
        "X_train = X_train/255.0\n",
        "X_test = X_test/255.0\n",
        "\n",
        "# Build the classification model\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "      tf.keras.layers.Flatten(),\n",
        "      keras.layers.Dense(128, activation='relu'),\n",
        "      keras.layers.Dense(256, activation='relu'),\n",
        "      keras.layers.Dense(10, activation='softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit( X_train, y_train, epochs=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RAgYhxaeucOy",
        "outputId": "05a487b0-56f6-48e2-a7fe-760d8b952d91"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.8256 - accuracy: 0.6942\n",
            "Epoch 2/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5424 - accuracy: 0.8024\n",
            "Epoch 3/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4783 - accuracy: 0.8288\n",
            "Epoch 4/1000\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4407 - accuracy: 0.8401\n",
            "Epoch 5/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4169 - accuracy: 0.8500\n",
            "Epoch 6/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3979 - accuracy: 0.8566\n",
            "Epoch 7/1000\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3834 - accuracy: 0.8622\n",
            "Epoch 8/1000\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3702 - accuracy: 0.8662\n",
            "Epoch 9/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3576 - accuracy: 0.8701\n",
            "Epoch 10/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3468 - accuracy: 0.8732\n",
            "Epoch 11/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3375 - accuracy: 0.8766\n",
            "Epoch 12/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3297 - accuracy: 0.8800\n",
            "Epoch 13/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3210 - accuracy: 0.8825\n",
            "Epoch 14/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.3138 - accuracy: 0.8857\n",
            "Epoch 15/1000\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3052 - accuracy: 0.8875\n",
            "Epoch 16/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2999 - accuracy: 0.8906\n",
            "Epoch 17/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2945 - accuracy: 0.8919\n",
            "Epoch 18/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2868 - accuracy: 0.8943\n",
            "Epoch 19/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2828 - accuracy: 0.8953\n",
            "Epoch 20/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2769 - accuracy: 0.8980\n",
            "Epoch 21/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2728 - accuracy: 0.8993\n",
            "Epoch 22/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2674 - accuracy: 0.9008\n",
            "Epoch 23/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2644 - accuracy: 0.9023\n",
            "Epoch 24/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2594 - accuracy: 0.9033\n",
            "Epoch 25/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2548 - accuracy: 0.9043\n",
            "Epoch 26/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2509 - accuracy: 0.9069\n",
            "Epoch 27/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2468 - accuracy: 0.9077\n",
            "Epoch 28/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2433 - accuracy: 0.9087\n",
            "Epoch 29/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2404 - accuracy: 0.9096\n",
            "Epoch 30/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2359 - accuracy: 0.9113\n",
            "Epoch 31/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2332 - accuracy: 0.9141\n",
            "Epoch 32/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2299 - accuracy: 0.9144\n",
            "Epoch 33/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2264 - accuracy: 0.9153\n",
            "Epoch 34/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2243 - accuracy: 0.9158\n",
            "Epoch 35/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2199 - accuracy: 0.9192\n",
            "Epoch 36/1000\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2178 - accuracy: 0.9180\n",
            "Epoch 37/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2143 - accuracy: 0.9194\n",
            "Epoch 38/1000\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2122 - accuracy: 0.9206\n",
            "Epoch 39/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2095 - accuracy: 0.9225\n",
            "Epoch 40/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2063 - accuracy: 0.9226\n",
            "Epoch 41/1000\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.2039 - accuracy: 0.9240\n",
            "Epoch 42/1000\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2009 - accuracy: 0.9241\n",
            "Epoch 43/1000\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.1980 - accuracy: 0.9258\n",
            "Epoch 44/1000\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1967 - accuracy: 0.9251\n",
            "Epoch 45/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1934 - accuracy: 0.9269\n",
            "Epoch 46/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1910 - accuracy: 0.9284\n",
            "Epoch 47/1000\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1888 - accuracy: 0.9305\n",
            "Epoch 48/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1854 - accuracy: 0.9311\n",
            "Epoch 49/1000\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1842 - accuracy: 0.9312\n",
            "Epoch 50/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1812 - accuracy: 0.9319\n",
            "Epoch 51/1000\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1789 - accuracy: 0.9334\n",
            "Epoch 52/1000\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1779 - accuracy: 0.9333\n",
            "Epoch 53/1000\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1759 - accuracy: 0.9339\n",
            "Epoch 54/1000\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1727 - accuracy: 0.9347\n",
            "Epoch 55/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1713 - accuracy: 0.9355\n",
            "Epoch 56/1000\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.1689 - accuracy: 0.9370\n",
            "Epoch 57/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1659 - accuracy: 0.9369\n",
            "Epoch 58/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1654 - accuracy: 0.9387\n",
            "Epoch 59/1000\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1624 - accuracy: 0.9387\n",
            "Epoch 60/1000\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1603 - accuracy: 0.9398\n",
            "Epoch 61/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1592 - accuracy: 0.9404\n",
            "Epoch 62/1000\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1578 - accuracy: 0.9413\n",
            "Epoch 63/1000\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1550 - accuracy: 0.9426\n",
            "Epoch 64/1000\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1528 - accuracy: 0.9429\n",
            "Epoch 65/1000\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1519 - accuracy: 0.9425\n",
            "Epoch 66/1000\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.1482 - accuracy: 0.9445\n",
            "Epoch 67/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1476 - accuracy: 0.9452\n",
            "Epoch 68/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1454 - accuracy: 0.9454\n",
            "Epoch 69/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1436 - accuracy: 0.9459\n",
            "Epoch 70/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1410 - accuracy: 0.9480\n",
            "Epoch 71/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1418 - accuracy: 0.9463\n",
            "Epoch 72/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1390 - accuracy: 0.9478\n",
            "Epoch 73/1000\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1378 - accuracy: 0.9482\n",
            "Epoch 74/1000\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1350 - accuracy: 0.9493\n",
            "Epoch 75/1000\n",
            "1875/1875 [==============================] - 18s 9ms/step - loss: 0.1340 - accuracy: 0.9494\n",
            "Epoch 76/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1311 - accuracy: 0.9503\n",
            "Epoch 77/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1304 - accuracy: 0.9506\n",
            "Epoch 78/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1298 - accuracy: 0.9518\n",
            "Epoch 79/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1279 - accuracy: 0.9508\n",
            "Epoch 80/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1252 - accuracy: 0.9534\n",
            "Epoch 81/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1247 - accuracy: 0.9528\n",
            "Epoch 82/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1234 - accuracy: 0.9532\n",
            "Epoch 83/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1215 - accuracy: 0.9545\n",
            "Epoch 84/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1218 - accuracy: 0.9549\n",
            "Epoch 85/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1186 - accuracy: 0.9557\n",
            "Epoch 86/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1175 - accuracy: 0.9565\n",
            "Epoch 87/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1153 - accuracy: 0.9571\n",
            "Epoch 88/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1148 - accuracy: 0.9571\n",
            "Epoch 89/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1125 - accuracy: 0.9574\n",
            "Epoch 90/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1133 - accuracy: 0.9579\n",
            "Epoch 91/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1108 - accuracy: 0.9587\n",
            "Epoch 92/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1095 - accuracy: 0.9584\n",
            "Epoch 93/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1079 - accuracy: 0.9591\n",
            "Epoch 94/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1054 - accuracy: 0.9605\n",
            "Epoch 95/1000\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1056 - accuracy: 0.9614\n",
            "Epoch 96/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1046 - accuracy: 0.9613\n",
            "Epoch 97/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1023 - accuracy: 0.9613\n",
            "Epoch 98/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1021 - accuracy: 0.9617\n",
            "Epoch 99/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1002 - accuracy: 0.9625\n",
            "Epoch 100/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0989 - accuracy: 0.9635\n",
            "Epoch 101/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0978 - accuracy: 0.9632\n",
            "Epoch 102/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0962 - accuracy: 0.9651\n",
            "Epoch 103/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0959 - accuracy: 0.9637\n",
            "Epoch 104/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0960 - accuracy: 0.9646\n",
            "Epoch 105/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0942 - accuracy: 0.9647\n",
            "Epoch 106/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0917 - accuracy: 0.9660\n",
            "Epoch 107/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0905 - accuracy: 0.9660\n",
            "Epoch 108/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0912 - accuracy: 0.9664\n",
            "Epoch 109/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0892 - accuracy: 0.9672\n",
            "Epoch 110/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0874 - accuracy: 0.9684\n",
            "Epoch 111/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0855 - accuracy: 0.9680\n",
            "Epoch 112/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0873 - accuracy: 0.9676\n",
            "Epoch 113/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0855 - accuracy: 0.9682\n",
            "Epoch 114/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0822 - accuracy: 0.9694\n",
            "Epoch 115/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0819 - accuracy: 0.9692\n",
            "Epoch 116/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0823 - accuracy: 0.9697\n",
            "Epoch 117/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0810 - accuracy: 0.9707\n",
            "Epoch 118/1000\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0785 - accuracy: 0.9716\n",
            "Epoch 119/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0784 - accuracy: 0.9701\n",
            "Epoch 120/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0784 - accuracy: 0.9704\n",
            "Epoch 121/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0771 - accuracy: 0.9714\n",
            "Epoch 122/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0761 - accuracy: 0.9717\n",
            "Epoch 123/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0748 - accuracy: 0.9720\n",
            "Epoch 124/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0738 - accuracy: 0.9721\n",
            "Epoch 125/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0731 - accuracy: 0.9732\n",
            "Epoch 126/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0717 - accuracy: 0.9734\n",
            "Epoch 127/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0714 - accuracy: 0.9733\n",
            "Epoch 128/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0708 - accuracy: 0.9740\n",
            "Epoch 129/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0711 - accuracy: 0.9740\n",
            "Epoch 130/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0682 - accuracy: 0.9745\n",
            "Epoch 131/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0667 - accuracy: 0.9755\n",
            "Epoch 132/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0672 - accuracy: 0.9758\n",
            "Epoch 133/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0664 - accuracy: 0.9751\n",
            "Epoch 134/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0638 - accuracy: 0.9767\n",
            "Epoch 135/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0641 - accuracy: 0.9762\n",
            "Epoch 136/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0649 - accuracy: 0.9761\n",
            "Epoch 137/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0635 - accuracy: 0.9772\n",
            "Epoch 138/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0623 - accuracy: 0.9770\n",
            "Epoch 139/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0631 - accuracy: 0.9755\n",
            "Epoch 140/1000\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0619 - accuracy: 0.9773\n",
            "Epoch 141/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0569 - accuracy: 0.9792\n",
            "Epoch 142/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0600 - accuracy: 0.9778\n",
            "Epoch 143/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0582 - accuracy: 0.9786\n",
            "Epoch 144/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0561 - accuracy: 0.9797\n",
            "Epoch 145/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0590 - accuracy: 0.9780\n",
            "Epoch 146/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0568 - accuracy: 0.9791\n",
            "Epoch 147/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0554 - accuracy: 0.9799\n",
            "Epoch 148/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0543 - accuracy: 0.9799\n",
            "Epoch 149/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0565 - accuracy: 0.9795\n",
            "Epoch 150/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0539 - accuracy: 0.9801\n",
            "Epoch 151/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0528 - accuracy: 0.9811\n",
            "Epoch 152/1000\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0509 - accuracy: 0.9813\n",
            "Epoch 153/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0534 - accuracy: 0.9803\n",
            "Epoch 154/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0507 - accuracy: 0.9815\n",
            "Epoch 155/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0528 - accuracy: 0.9804\n",
            "Epoch 156/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0502 - accuracy: 0.9813\n",
            "Epoch 157/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0486 - accuracy: 0.9825\n",
            "Epoch 158/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0477 - accuracy: 0.9826\n",
            "Epoch 159/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0484 - accuracy: 0.9822\n",
            "Epoch 160/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0480 - accuracy: 0.9826\n",
            "Epoch 161/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0473 - accuracy: 0.9830\n",
            "Epoch 162/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0478 - accuracy: 0.9827\n",
            "Epoch 163/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0470 - accuracy: 0.9829\n",
            "Epoch 164/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0449 - accuracy: 0.9837\n",
            "Epoch 165/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0458 - accuracy: 0.9835\n",
            "Epoch 166/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0438 - accuracy: 0.9840\n",
            "Epoch 167/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0441 - accuracy: 0.9840\n",
            "Epoch 168/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0453 - accuracy: 0.9833\n",
            "Epoch 169/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0406 - accuracy: 0.9850\n",
            "Epoch 170/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0446 - accuracy: 0.9839\n",
            "Epoch 171/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0428 - accuracy: 0.9847\n",
            "Epoch 172/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0431 - accuracy: 0.9844\n",
            "Epoch 173/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0403 - accuracy: 0.9852\n",
            "Epoch 174/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0404 - accuracy: 0.9854\n",
            "Epoch 175/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0401 - accuracy: 0.9856\n",
            "Epoch 176/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0402 - accuracy: 0.9854\n",
            "Epoch 177/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0395 - accuracy: 0.9856\n",
            "Epoch 178/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0407 - accuracy: 0.9846\n",
            "Epoch 179/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0378 - accuracy: 0.9864\n",
            "Epoch 180/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0385 - accuracy: 0.9859\n",
            "Epoch 181/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0406 - accuracy: 0.9859\n",
            "Epoch 182/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0367 - accuracy: 0.9868\n",
            "Epoch 183/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0367 - accuracy: 0.9870\n",
            "Epoch 184/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0378 - accuracy: 0.9862\n",
            "Epoch 185/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0353 - accuracy: 0.9872\n",
            "Epoch 186/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0360 - accuracy: 0.9869\n",
            "Epoch 187/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0360 - accuracy: 0.9869\n",
            "Epoch 188/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0353 - accuracy: 0.9880\n",
            "Epoch 189/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0362 - accuracy: 0.9868\n",
            "Epoch 190/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0354 - accuracy: 0.9873\n",
            "Epoch 191/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0331 - accuracy: 0.9884\n",
            "Epoch 192/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0342 - accuracy: 0.9882\n",
            "Epoch 193/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0326 - accuracy: 0.9882\n",
            "Epoch 194/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0332 - accuracy: 0.9881\n",
            "Epoch 195/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0338 - accuracy: 0.9881\n",
            "Epoch 196/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0328 - accuracy: 0.9883\n",
            "Epoch 197/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0322 - accuracy: 0.9888\n",
            "Epoch 198/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0293 - accuracy: 0.9897\n",
            "Epoch 199/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0326 - accuracy: 0.9883\n",
            "Epoch 200/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0315 - accuracy: 0.9884\n",
            "Epoch 201/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0316 - accuracy: 0.9886\n",
            "Epoch 202/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0298 - accuracy: 0.9897\n",
            "Epoch 203/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0319 - accuracy: 0.9882\n",
            "Epoch 204/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0293 - accuracy: 0.9891\n",
            "Epoch 205/1000\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0311 - accuracy: 0.9888\n",
            "Epoch 206/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0276 - accuracy: 0.9900\n",
            "Epoch 207/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0282 - accuracy: 0.9900\n",
            "Epoch 208/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0320 - accuracy: 0.9882\n",
            "Epoch 209/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0278 - accuracy: 0.9903\n",
            "Epoch 210/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0287 - accuracy: 0.9901\n",
            "Epoch 211/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0254 - accuracy: 0.9912\n",
            "Epoch 212/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0302 - accuracy: 0.9895\n",
            "Epoch 213/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0271 - accuracy: 0.9905\n",
            "Epoch 214/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0291 - accuracy: 0.9897\n",
            "Epoch 215/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0282 - accuracy: 0.9900\n",
            "Epoch 216/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0265 - accuracy: 0.9907\n",
            "Epoch 217/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0265 - accuracy: 0.9903\n",
            "Epoch 218/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0268 - accuracy: 0.9903\n",
            "Epoch 219/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0260 - accuracy: 0.9907\n",
            "Epoch 220/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0292 - accuracy: 0.9898\n",
            "Epoch 221/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0254 - accuracy: 0.9914\n",
            "Epoch 222/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0240 - accuracy: 0.9919\n",
            "Epoch 223/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0248 - accuracy: 0.9915\n",
            "Epoch 224/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0246 - accuracy: 0.9917\n",
            "Epoch 225/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0259 - accuracy: 0.9911\n",
            "Epoch 226/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0275 - accuracy: 0.9906\n",
            "Epoch 227/1000\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0238 - accuracy: 0.9915\n",
            "Epoch 228/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0273 - accuracy: 0.9903\n",
            "Epoch 229/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0208 - accuracy: 0.9930\n",
            "Epoch 230/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0254 - accuracy: 0.9908\n",
            "Epoch 231/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0265 - accuracy: 0.9908\n",
            "Epoch 232/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0251 - accuracy: 0.9913\n",
            "Epoch 233/1000\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0225 - accuracy: 0.9920\n",
            "Epoch 234/1000\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0240 - accuracy: 0.9916\n",
            "Epoch 235/1000\n",
            "1666/1875 [=========================>....] - ETA: 0s - loss: 0.0245 - accuracy: 0.9919"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6ca0f7cf60e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[0;32m-> 2452\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2687\u001b[0m           f\"hashable.  Original error: {e}.\")\n\u001b[1;32m   2688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2689\u001b[0;31m     \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2690\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/core/function/polymorphism/function_cache.py\u001b[0m in \u001b[0;36mlookup\u001b[0;34m(self, key, use_function_subtyping)\u001b[0m\n\u001b[1;32m    117\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_primary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mdispatch_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdispatch_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_primary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdispatch_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/core/function/polymorphism/type_dispatch.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;34m\"\"\"Returns the deepest subtype target if it exists in the table.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# For known exact matches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_table\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/core/function/polymorphism/function_cache.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/core/function/trace_type/default_types.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollection_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/core/function/trace_type/default_types.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollection_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/core/function/trace_type/default_types.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m__get_cmp_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;34m\"\"\"Returns a hashable eq-comparable key for `self`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;31m# TODO(b/133606651): Decide whether to cache this value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m__make_cmp_key\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    515\u001b[0m       ])\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    515\u001b[0m       ])\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on unseen data\n",
        "model.evaluate( X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9QijY2ivztP",
        "outputId": "13716d73-3a9a-49d4-9097-4bfbf1afa546"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9349 - accuracy: 0.8842\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9349417686462402, 0.8841999769210815]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try Callback\n",
        "\n",
        "class myCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy') is not None and logs.get('accuracy') >= 0.8): \n",
        "      print('\\nReached 80% accuracy so cancelling training!')\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "fmnist = keras.datasets.fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fmnist.load_data()\n",
        "\n",
        "X_train = X_train/255.0\n",
        "X_test = X_test/255.0\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(258, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=20, callbacks=[callbacks])\n",
        "\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ricu8dYydC3",
        "outputId": "707931b3-5223-4b00-c0ef-f850947711b0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1861/1875 [============================>.] - ETA: 0s - loss: 0.4735 - accuracy: 0.8290\n",
            "Reached 80% accuracy so cancelling training!\n",
            "1875/1875 [==============================] - 33s 4ms/step - loss: 0.4726 - accuracy: 0.8293\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4137 - accuracy: 0.8475\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4137140214443207, 0.8475000262260437]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EQoxlvvi03Nf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}